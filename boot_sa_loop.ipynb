{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1dd1f87-6dc4-4ede-a2b2-eaf48b1c1cd1",
   "metadata": {},
   "source": [
    "# Bootstrap from training samplt for sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a707e6-8585-43d2-abd2-c0196d8f3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from algo import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import LeaveOneOut,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nibabel as nib\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089f954-7b15-403d-a10c-7faee75f43dc",
   "metadata": {},
   "source": [
    "# Data preparition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f888fa-12f3-411a-bd6f-05069d15c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "path = '/home/jupyter-amyang/data/attention_signature/train_test_data'\n",
    "\n",
    "# read fa train and test data, also test on sa\n",
    "train_X_sa = np.load(os.path.join(path,'train_X_sa.npy'))\n",
    "test_X_sa = np.load(os.path.join(path,'test_X_sa.npy'))\n",
    "train_Y_sa = np.load(os.path.join(path,'train_Y_sa.npy'))\n",
    "test_Y_sa = np.load(os.path.join(path,'test_Y_sa.npy'))\n",
    "X_fa = np.load(os.path.join(path,'X_fa.npy'))\n",
    "Y_fa = np.load(os.path.join(path,'Y_fa.npy'))\n",
    "\n",
    "mask_NaN = np.load(os.path.join(path,'mask_NaN.npy'))\n",
    "\n",
    "# scale input \n",
    "test_X_sa = scale(test_X_sa)\n",
    "X_fa = scale(X_fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fb5b8-e884-4d11-80db-81813930d9d8",
   "metadata": {},
   "source": [
    "# Result holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2487d2-130c-4ec6-8cb5-50bfaf07864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_coef = np.zeros(mask_NaN.shape[0])\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "fa_accuracy = []\n",
    "hyper_c = []\n",
    "beta_cutoff_value = []\n",
    "pc_num = []\n",
    "concat_data = np.c_[train_X_sa,train_Y_sa]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fe17e-e678-4b18-9b88-3671b3e3f88b",
   "metadata": {},
   "source": [
    "# For loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d27782-6c15-45a1-8063-48a7b49d102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th calculation has been completed, using time 159.24211263656616s\n"
     ]
    }
   ],
   "source": [
    "boot_size = 1\n",
    "for i in range(boot_size):\n",
    "    start = time.time()\n",
    "    \n",
    "    # sample a new set of data \n",
    "    bootstraped_data = bootstrap_data(concat_data)\n",
    "    X = bootstraped_data[:,:-1] \n",
    "    Y = bootstraped_data[:,-1]\n",
    "    \n",
    "    # PCA\n",
    "    train_X_sa = scale(X)\n",
    "    train_Y_sa = Y # rename Y\n",
    "    \n",
    "    # reduce dimention with PCA\n",
    "    pca = PCA() # reduce dimentions to the number of observations\n",
    "    pca.fit(train_X_sa)\n",
    "    train_X_sa_pc = pca.transform(train_X_sa)\n",
    "    test_X_sa_pc = pca.transform(test_X_sa)\n",
    "    X_fa_pc = pca.transform(X_fa)\n",
    "    \n",
    "    # grid search for beta hyper-parameter\n",
    "    tuned_parameters = [{'C': np.arange(0.05,1.05,0.05)}]\n",
    "    clf = GridSearchCV(LogisticRegression(penalty='l1',solver='liblinear'),tuned_parameters,cv=10,scoring='f1')\n",
    "    clf.fit(train_X_sa_pc, train_Y_sa)\n",
    "\n",
    "    C_best = clf.best_params_['C']\n",
    "    hyper_c.append(C_best)\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l1',solver='liblinear',C=C_best) # C value determined by grid search\n",
    "    clf.fit(train_X_sa_pc,train_Y_sa)\n",
    "    LR_beta_mean = clf.coef_[0]\n",
    "    \n",
    "    # gradually rule out PCs with low beta\n",
    "    PC_record = {} # dictionary to store predicition accuracy with each phase of reduction of PCs\n",
    "    for beta_cutoff in np.arange(0,np.max(abs(LR_beta_mean)),0.001):\n",
    "        mask_beta = (abs(LR_beta_mean) > beta_cutoff)\n",
    "        train_X_sa_masked = train_X_sa_pc[:,mask_beta]\n",
    "        test_X_sa_masked = test_X_sa_pc[:,mask_beta]\n",
    "    \n",
    "        clf.fit(train_X_sa_masked,train_Y_sa)\n",
    "    \n",
    "        acc = clf.score(test_X_sa_masked,test_Y_sa)\n",
    "        PC_record[str(beta_cutoff)] = acc\n",
    "    \n",
    "    pc_pd = pd.DataFrame({'beta_cutoff':PC_record.keys(),\n",
    "                     'accuracy':PC_record.values()})\n",
    "    \n",
    "    index = pc_pd['accuracy'].argmax()\n",
    "    beta_cutoff_optimized = pc_pd['beta_cutoff'][index]\n",
    "    beta_cutoff_optimized = float(beta_cutoff_optimized)\n",
    "    beta_cutoff_value.append(beta_cutoff_optimized)\n",
    "    \n",
    "    # train logistic regression with whole training data \n",
    "    \n",
    "    # employ new X with mask of optimized beta cutoff\n",
    "    mask_beta_optimized = (abs(LR_beta_mean) > beta_cutoff_optimized)\n",
    "    pc_num.append(np.sum(mask_beta_optimized))\n",
    "    train_X_sa_optimized = train_X_sa_pc[:,mask_beta_optimized]\n",
    "    test_X_sa_optimized = test_X_sa_pc[:,mask_beta_optimized]\n",
    "    X_fa_optimized = X_fa_pc[:,mask_beta_optimized]\n",
    "\n",
    "    clf.fit(train_X_sa_optimized,train_Y_sa)\n",
    "    \n",
    "    # get beta and transfer back to mni size\n",
    "    coef = clf.coef_[0]\n",
    "    coef_modified = mask_back(mask_beta_optimized,coef,mask_type='beta')\n",
    "    inverse_coef= pca.inverse_transform(coef_modified)\n",
    "    coef_mni = mask_back(mask_NaN,inverse_coef,mask_type='mni')\n",
    "    boot_coef = np.vstack((boot_coef,coef_mni))\n",
    "    \n",
    "    # store accuracy \n",
    "    train_accuracy.append(clf.score(train_X_sa_optimized,train_Y_sa))\n",
    "    test_accuracy.append(clf.score(test_X_sa_optimized,test_Y_sa))\n",
    "    fa_accuracy.append(clf.score(X_fa_optimized,Y_fa))\n",
    "    \n",
    "    # report \n",
    "    end = time.time()\n",
    "    print(f'The {i}th calculation has been completed, using time {end-start}s')\n",
    "\n",
    "boot_coef = boot_coef[1:,:]\n",
    "\n",
    "# dataFrame the prediction accuracy \n",
    "result_matrix = np.c_[np.array(train_accuracy),np.array(test_accuracy),np.array(fa_accuracy),\n",
    "                      np.array(hyper_c),np.array(beta_cutoff_value),np.array(pc_num)]\n",
    "df_result = pd.DataFrame(result_matrix,columns=['train_accuracy','test_accuracy','fa_accuracy',\n",
    "                                               'hyper_c','beta_cutoff','pc_num'])\n",
    "\n",
    "# save_data\n",
    "save_path = '/home/jupyter-amyang/workingdir/attention_results/bootstrap/sa'\n",
    "np.save(os.path.join(save_path,'beta_train_sa.npy'),boot_coef)\n",
    "df_result.to_csv(os.path.join(save_path,'train_sa_accuracy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68d4ec-9bf1-437a-acdd-ff4f50721b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
